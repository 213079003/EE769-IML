# -*- coding: utf-8 -*-
"""213070015_213079003_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vUMxP1IMOKzb2auCKnViihV94n7QGbuc

#**EE769 Introduction to Machine Learning**

#Assignment 1: Gradient Descent, Linear Regression, and Regularization


**Template and Instructions**



1. Up to two people can team up, but only one should submit, and both should understand the entire code.
2. Every line of code should end in a comment explaining the line
3. It is recommended to solve the assignment in Google Colab.
Write your roll no.s separated by commas here: 213070015, 213079003
4. Write your names here: Satyabrata Sahu, Akella Venkata Surya Srinidhi
5. There are two parts to the assignment. In the Part 1, the code format has to be strictly followed to enable auto-grading. In the second part, you can be creative.
6. **You can discuss with other groups or refer to the internet without being penalized, but you cannot copy their code and modify it. Write every line of code and comment on your own.**

#**Part 1 begins ...**
**Instructions to be strictly followed:**

1. Do not add any code cells or markdown cells until the end of this part. Especially, do not change the blocks that say "TEST CASES, DO NOT CHANGE"
2. In all other cells only add code where it says "CODE HERE".
3. If you encounter any raise NotImplementedError() calls you may comment them out.

We cannot ensure correct grading if you change anything else, and you may be penalised for not following these instructions.

## Import Statements
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""## Normalize function 


"""

def Normalize(X): # Output should be a normalized data matrix of the same dimension
    '''
    Normalize all columns of X using mean and standard deviation
    '''
    # YOUR CODE HERE
    """
      X : data matrix which is to normalised  
          norm_X : normalized data matrix

         A number is normalised with zero mean and unit variance as follows:
                  Z = (X - μ)/σ 
        where,  μ = Mean(X)
                σ = Standard Deviation(X)  
    """
  
    norm_X = (X - X.mean(axis=0))/X.std(axis=0)

    # X.mean(axis=0) computes the mean along rows i.e returns a row vector of means of each column of X
    # X.std(axis=0) computes the standard deviation along rows i.e returns a row vector of std_dev of each column of X
    return norm_X

    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - 1 dimensional array'''
#X=np.array([[1,2,3],[3,4,5],[7,8,9]])
X1=np.array([1,2,3])
np.testing.assert_array_almost_equal(Normalize(X1),np.array([-1.224,  0.      ,  1.224]),decimal=3)
''' case 2 - 2 dimensional array'''
X2=np.array([[4,7,6],[3,8,9],[5,11,10]])
np.testing.assert_array_almost_equal(Normalize(X2),np.array([[ 0.  , -0.980581, -1.372813],[-1.224745, -0.392232,  0.392232],[ 1.224745,  1.372813,  0.980581]]))
''' case 3 - 1 dimensional array with float'''
X3=np.array([5.5,6.7,3.2,6.7])
np.testing.assert_array_almost_equal(Normalize(X3),np.array([-0.017,  0.822, -1.627,  0.822]),decimal=3)

"""## Prediction Function

Given X and w, compute the predicted output. Do not forget to add 1's in X
"""

def Prediction (X, w): # Output should be a prediction vector y
    '''
    Compute Prediction given an input datamatrix X and weight vecor w. Output y = [X 1]w where 1 is a vector of all 1s 
    '''
    # YOUR CODE HERE
    # we need to stack a column of 1's to the datamatrix X
    # np.ones((X.shape[0],1)) creates a column vector of 1's with number of rows = number of rows in X
    # np.concatenate(X1, X2, axis=1) stacks X2 to X1 as columns
    
    X1 = np.concatenate((X,np.ones((X.shape[0],1))),axis=1) # creating X stacked with column of 1's i.e. [X 1] matrix/vector and storing as X1
    y = np.dot(X1,w)                                        # np.dot() performs the element wise multiplication of the vectors/matrices given    
    return y 
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - Known input output matrix and weights 1'''
X1 = np.array([[3,2],[1,1]])
w1 = np.array([2,1,1]) 
np.testing.assert_array_equal(Prediction(X1,w1),np.array([9,4]))

"""## Loss Functions

Code the four  loss functions:

1. MSE loss is only for the error
2. MAE loss is only for the error
3. L2 loss is for MSE and L2 regularization, and can call MSE loss
4. L1 loss is for MSE and L1 regularization, and can call MSE loss
"""

def MSE_Loss (X, t, w, lamda =0): # Ouput should be a single number
    '''
    lamda=0 is a default argument to prevent errors if you pass lamda to a function that doesn't need it by mistake. 
    This allows us to call all loss functions with the same input format.
    
    You are encouraged read about default arguments by yourself online if you're not familiar.
    '''
    # YOUR CODE HERE
    """ 
    Mean squared error (MSE) is the average squared difference between the observed and predicted values. 
    It is given by the formula:
                  MSE  = (Σ(yi -ŷi)^2)/n
    where, yi is the ith observed value.
           ŷi is the corresponding predicted value.
           n = the number of observations.
           (yi -ŷi)^2 = squared difference between yi, ŷi
    """
    y = Prediction(X,w)                         # finding the predicted values
    m = y.size                                  # finding the number of observations (y.size gives the numbers of elements in y)
    mse_loss = (1/m)*(np.sum(np.square(t-y)))   # t--> observed values and computing the mse loss as per formula
    return mse_loss
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MSE_Loss(X,t,w),0.53,decimal=3)

def MAE_Loss (X, t, w, lamda = 0): # Output should be a single number
    # YOUR CODE HERE
    """ 
        Mean absolute error (MAE) is the average absolute difference between the observed and predicted values. 
        It is given by the formula:
                      MAE  = (Σ|(yi -ŷi)|)/n
        where, yi is the ith observed value.
               ŷi is the corresponding predicted value.
               n = the number of observations.
              |(yi -ŷi)| =  absolute difference between yi, ŷi
    """
    y = Prediction(X,w)                   # finding the predicted values
    m = y.size                            # finding the number of observations (y.size gives the numbers of elements in y)
    mae_loss = (1/m)*np.sum(np.abs(t-y))  # t--> observed values and computing the mae loss as per formula
    return mae_loss
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MAE_Loss(X,t,w),0.700,decimal=3)

def L2_Loss (X, t, w, lamda): # Output should be a single number based on L2-norm (with sqrt)
    ''' Need to specify what inputs are'''
    # YOUR CODE HERE

    """ 
    A linear regression model that implements L2 norm for regularisation is called Ridge Regression. 
    The cost function with L2 regularization is given as:
          Cost = MSE Loss + λ(L2 Norm of w)
    where MSE Loss = (Σ(yi -ŷi)^2)/n is the mse loss between the observed and predicted values
          L2 Norm of w =  √(Σ(wi^2)) = ||w|| i.e square root of squared value of weights(wi) --> L2 regularization element.
          λ = Regularization parameter

    """    
    mse = MSE_Loss (X, t, w, lamda =0)          # computing the mse loss by calling the MSE_Loss() function defined previously
    l2norm = np.sqrt(np.sum(np.square(w[:-1]))) # computing the l2-norm of weights excluding the bias term(last term of w-->w[-1] is bias)
    l2_loss = mse + lamda*l2norm                # finding the net cost 
    return l2_loss
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L2_Loss(X,t,w,0.5),1.675,decimal=3)

def L1_Loss (X, t, w, lamda): # Output should be a single number
    # YOUR CODE HERE
    """ 
    A linear regression model that implements L1 norm for regularisation is called Lasso Regression. 
    The cost function with L2 regularization is given as:
          Cost = MSE Loss + λ(L1 Norm of w)
    where MSE Loss = (Σ(yi -ŷi)^2)/n is the mse loss between the observed and predicted values
          L1 Norm of w =  Σ|wi| i.e sum of absolute value of weights(wi) --> L1 regularization element.
          λ = Regularization parameter

    """  
    mse = MSE_Loss (X, t, w, lamda =0) # computing the mse loss by calling the MSE_Loss() function defined previously
    l1norm = np.sum(np.abs(w[:-1]))    # computing the l1-norm of weights excluding the bias term(last term of w-->w[-1] is bias)
    l1_loss = mse + lamda*l1norm       # finding the net cost 
    return l1_loss
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L1_Loss(X,t,w,0.5),2.280,decimal=3)

def NRMSE_Metric (X, t, w, lamda=0): # Output should be a single number. RMSE/std_dev(t)
    # YOUR CODE HERE
    """  
      Normalizing the RMSE facilitates the comparison between datasets or models with different scales.
      This is achieved by the following formula:
              NRMSE = RMSE/σ  = (√(MSE))/ σ
          where, we are normalizing by σ
                  σ = Standard Deviation
                RMSE = Root Mean Squared Error
    """
    mse = MSE_Loss (X, t, w, lamda =0)          # computing the mse loss by calling the MSE_Loss() function defined previously
    nrmse_loss = np.sqrt(mse)/(np.sum(t.std())) # np.sum(t.std()) gives the std_dev of target variable and np.sqrt(mse) gives the rmse
    return nrmse_loss
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' Test case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(NRMSE_Metric(X,t,w,0.5),0.970,decimal=3)

"""## Gradient function
Each Loss function will have its own gradient function:

1. MSE gradient is only for the error
2. MAE gradient is only for the error
3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient
4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient
"""

def MSE_Gradient (X, t, w, lamda=0): # Output should have the same size as w
    # YOUR CODE HERE
    """ 
    The gradient is nothing but derivative. Since our parameters are w(weights),b(bias), we take the derivative w.r.t to w,b
    The MSE gradient is obtained by finding the derivative of the mse cost function w.r.t w,b
        Mse Cost function MSE  = (Σ(ŷi-yi)^2)/n 
        where ŷi = (wi)(xi) + b
      --> Gradient w.r.t w is d(MSE)/dw and is given by:
            d(MSE)/dw = d((Σ(ŷi-yi)^2)/n)/dw = (2/n)*(Σ(ŷi-yi)*xi) = (2/n)*(Σ(errori*xi) 
        where errori = error between ith predicted and observed values = ŷi-yi
      --> Gradient w.r.t b is d(MSE)/db and is given by:
            d(MSE)/db = d((Σ(ŷi-yi)^2)/n)/db = (2/n)*(Σ(ŷi-yi)) = (2/n)* Σ(errori)  = (2/n)* sum of errors

    """
    y_estimated = Prediction(X, w)          # finds the predicted value by calling Prediction() the function defined previously
    error = (y_estimated - t)               # find error between predicted and observed values
    
    # here w contains the weights included with bias term.
    # so here find the gradient vector w.r.t w and then replace the last element with gradient component w.r.t bias since w[-1] is bias 
    
    gradient = (2 / X.shape[0]) * np.dot(X.T,error)                      # finding gradient vector w.r.t w
    gradient = np.insert(gradient,X.shape[1],(2/X.shape[0])*error.sum()) # replacing the last term with grad w.r.t b
    return gradient
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MSE_Gradient(X,t,w),np.array([2.55, 2.94, 2.9 , 0.4 ]),decimal=3)

def MAE_Gradient (X, t, w, lamda=0): # Output should have the same size as w
    # YOUR CODE HERE
    """ 
    The MAE gradient is obtained by finding the derivative of the mae cost function w.r.t w,b
        Mae Cost function MAE  = (Σ|(yi -ŷi)|)/n
        where ŷi = (wi)(xi)+b
      --> Gradient w.r.t w is d(MAE)/dw and is given by:
            d(MAE)/dw = d((Σ|(yi -ŷi)|)/n)/dw =  (1/n)* Σ(yi -ŷi/|yi -ŷi|)*xi = (1/n)* Σ(errori/abs(errori)*xi)   (Since d(|x|)/dx = x/|x|)
        where errori = error between ith predicted and observed values = ŷi-yi
      --> Gradient w.r.t b is d(MAE)/db and is given by:
            d(MAE)/db = d((Σ|(yi -ŷi)|)/n)/db = (1/n)*Σ(yi -ŷi/|yi -ŷi|) = (1/n)* Σ(errori/abs(errori)) 

    """   
    y_estimate=Prediction(X,w)               # finds the predicted value by calling Prediction() the function defined previously
    error = (y_estimate - t)                 # find error between predicted and observed values
    
    # here w contains the weights included with bias term.
    # so here find the gradient vector w.r.t w and then replace the last element with gradient component w.r.t bias since w[-1] is bias 

    gradient = (1 / X.shape[0]) * np.dot((X).T,(error/abs(error)))                  # finding gradient vector w.r.t w
    gradient=np.insert(gradient,X.shape[1],(1/X.shape[0])*(error/abs(error)).sum()) # replacing the last term with grad w.r.t b
    return gradient
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MAE_Gradient(X,t,w),np.array([0.75,  0.3 ,  0.5 , 0.]),decimal=3)

def L2_Gradient (X, t, w, lamda): # Output should have the same size as w
    # YOUR CODE HERE
    """  
    The L2 gradient is obtained by finding the derivative of the l2loss cost function w.r.t w,b
        L2 Cost function L2_Loss  = MSE Loss + λ(L2 Norm of w) 
        where ŷi = (wi)(xi)+b
      --> Gradient w.r.t w is d(L2_Loss)/dw and is given by:
            d(L2_Loss)/dw =  d(MSE)/dw + λd(L2 Norm of w)/dw

          --> d(MSE)/dw = d((Σ(ŷi-yi)^2)/n)/dw = (2/n)*(Σ(ŷi-yi)*xi) = (2/n)*(Σ(errori*xi) 
                where errori = error between ith predicted and observed values = ŷi-yi
          --> d(L2 Norm of w)/dw = d(√(Σ(wi^2)))/dw = (1/√(Σ(wi^2)))* wi

      Thus, d(L2_Loss)/dw  = (2/n)*(Σ(errori*xi) + λ((1/√(Σ(wi^2)))* wi)
      --> Gradient w.r.t b is d(L2_Loss)/db and is given by:
          d(L2_Loss)/db =  d(MSE)/db + λd(L2 Norm of w)/db  

          --> d(MSE)/db = d((Σ(ŷi-yi)^2)/n)/db = (2/n)*(Σ(ŷi-yi)) = (2/n)* Σ(errori)  = (2/n)* sum of errors
          --> d(L2 Norm of w)/db = d(√(Σ(wi^2)))/db  = 0
      Thus,  d(L2_Loss)/db =  (2/n)* sum of errors

    """
    y_estimated=Prediction(X, w)                   # finds the predicted value by calling Prediction() the function defined previously
    error = (y_estimated - t)                      # find error between predicted and observed values
    
    # here w contains the weights included with bias term.
    # so here find the gradient vector w.r.t w and then replace the last element with gradient component w.r.t bias since w[-1] is bias     
    
    gradient = (2 / X.shape[0]) * np.dot(X.T,error)+ lamda*w[:-1]/np.sqrt(np.sum(np.square(w[:-1]))) # finding gradient vector w.r.t w 
    gradient=np.insert(gradient,X.shape[1],(2/X.shape[0])*error.sum())                      # replacing the last term with grad w.r.t b
    return gradient
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L2_Gradient(X,t,w,0.5),np.array([2.986, 2.721, 3.009 , 0.4 ]),decimal=3)

def L1_Gradient (X, t, w, lamda): # Output should have the same size as w
    # YOUR CODE HERE
    """  
    The L1 gradient is obtained by finding the derivative of the l1loss cost function w.r.t w,b
        L1 Cost function L1_Loss  = MSE Loss + λ(L1 Norm of w) 
        where ŷi = (wi)(xi)+b
      --> Gradient w.r.t w is d(L1_Loss)/dw and is given by:
            d(L1_Loss)/dw =  d(MSE)/dw + λd(L1 Norm of w)/dw

          --> d(MSE)/dw = d((Σ(ŷi-yi)^2)/n)/dw = (2/n)*(Σ(ŷi-yi)*xi) = (2/n)*(Σ(errori*xi) 
                where errori = error between ith predicted and observed values = ŷi-yi
          --> d(L1 Norm of w)/dw = d(Σ|wi|)/dw = wi/|wi|

      Thus, d(L1_Loss)/dw  = (2/n)*(Σ(errori*xi) + λ(wi/|wi|)
      --> Gradient w.r.t b is d(L2_Loss)/db and is given by:
          d(L1_Loss)/db =  d(MSE)/db + λd(L1 Norm of w)/db  

          --> d(MSE)/db = d((Σ(ŷi-yi)^2)/n)/db = (2/n)*(Σ(ŷi-yi)) = (2/n)* Σ(errori)  = (2/n)* sum of errors
          --> d(L1 Norm of w)/db = d(Σ|wi|)/db  = 0
      Thus,  d(L1_Loss)/db =  (2/n)* sum of errors

    """   
    y_estimated=Prediction(X, w)                   # finds the predicted value by calling Prediction() the function defined previously
    error = (y_estimated - t)                      # find error between predicted and observed values

    # here w contains the weights included with bias term.
    # so here find the gradient vector w.r.t w and then replace the last element with gradient component w.r.t bias since w[-1] is bias
        
    gradient = (2 / X.shape[0]) * np.dot(X.T,error) + lamda*w[:-1]/abs(w[:-1]) # finding gradient vector w.r.t w
    gradient=np.insert(gradient,X.shape[1],(2/X.shape[0])*error.sum())         # replacing the last term with grad w.r.t b
    return gradient
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L1_Gradient(X,t,w,0.5),np.array([3.05, 2.44, 3.4 , 0.4 ]),decimal=3)

"""## Gradient Descent Function

"""

def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement
    # YOUR CODE HERE
    """
    The Gradient Descent algorithm  is an iterative first-order algorithm of optimisation to find the local minima.
    It helps in minimizing the cost/loss function. It works in the following way:
      1) initialize the weights
      2) calculate gradient of cost function w.r.t weights
      3) scale with learning rate(η) and subtract it with the previous weights and update the weights
         i.e w--> w - (η * d(cost)/dw)
         The subtraction is done to move in opposite direction ( since we need to minimise)

      4) Repeat the steps 2 and 3 over each iteration until one of the criteria is met:
         a) either the difference between the cost calculated with previous weights and the cost calculated with updated 
            weights is less than epsilon(ε)
         b) the maximum of iterations are reached.
    """

    loss1 = 1e10  # Initialising the cost
    # iterating over each iteration upto the max number of iterations denoted by "max_iter"
    for _ in range(max_iter):
      # updating the weights by using the formula and storing as new weights(w1) where w are previous weights, lr is learning rate
      w1 = w - (lr *gradfunc(X, t, w, lamda=lamda))  
      
      loss2 = lossfunc(X, t, w1, lamda=lamda) # calculate the new cost with the updated weights
      
      if np.abs(loss2-loss1) < epsilon: # breaking condition where the difference between previous and new cost is less than ε
        break
      
      w = w1   # setting the current weights as previous weights and repeating the process
    
    w_final = w1      # saving the optimised weights 
    train_loss_final = lossfunc(X,t,w_final,lamda=lamda)                  # finding the training loss with the optimised weights  
    validation_loss_final = lossfunc(X_val, t_val, w_final,lamda=lamda)   # finding the validation loss with the optimised weights  
    validation_NRMSE = NRMSE_Metric (X_val, t_val, w_final)               # finding the validation NRMSE loss with the optimised weights
   
    return w_final, train_loss_final, validation_loss_final, validation_NRMSE #You should return variables structured like this
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
X=np.array([[23,24],[1,2]])
t=np.array([4,5])
X_val=np.array([[3,4],[5,6]])
t_val=np.array([3,4])
w=np.array([3,2,1])
results =Gradient_Descent (X, X_val, t, t_val, w, 0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient) 
np.testing.assert_allclose([results[1]],[697.919],rtol =0.05)
np.testing.assert_allclose([results[2]],[20],atol=5) # we expect around 17.5  but some students got 24 which we will also accept
#Instructor Values of results[1] and results [2] are 697.919 and 17.512 respectively

"""## Pseudo Inverse Method

You have to implement a slightly more advanced version, with L2 penalty:

w = (X' X + lambda I)^(-1) X' t.

See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf

Here, the column of 1's in assumed to be included in X
"""

def Pseudo_Inverse (X, t, lamda): # Output should be weight vector
    # YOUR CODE HERE
    """
    The weights for L2 Regularisation Linear Regression is obtained by finding the Pseudo-inverse Method.
    They are computed by using the below formula:
           w = (X'X + lambda I)^(-1) X't
        where X : data matrix with m sample points and n features(including the column of 1's) i.e mxn matrix
              X': transpose of X i.e of order nxm matrix
              I : identity matrix of order same as X'X (i.e nxn) in order to obey addition
          lambda: Regularisation Parameter
              t : the target variable of order mx1 
              w : weights vector of order nx1
    """
    X1 = np.concatenate((X,np.ones((X.shape[0],1))),axis=1) # creating X stacked with column of 1's i.e. [X 1] matrix/vector and storing as X1
    shape=np.dot(X1.T,X1).shape                             # getting the shape of X'X
    w=np.dot(np.dot(np.linalg.inv(np.dot(X1.T,X1)+lamda*np.identity(shape[0])),X1.T),t) # computing w using formula 
    return w
    
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - other data'''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
np.testing.assert_array_almost_equal(Pseudo_Inverse(X,t,0.5),np.array([ 0.491,  0.183,  0.319, -0.002]),decimal=3)

"""#... Part 1 ends Below this you be more creative. Just comment out the lines where you save files (e.g. test predictions).

#**Part 2 begins ...**

**Instructions to be loosely followed (except number 8):**

1. Add more code and text cells between this and the last cell.
2. Read training data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv only. Do not use a local copy of the dataset.
3. Find the best lamda for **MSE+lamda*L2(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
4. Find the best lamda for **MSE+lamda*L1(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
5. Find the best lamda for the **pseudo-inv method**. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
6. Write your observations and conclusions.
7. Read test data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv only. Do not use a local copy of the dataset. Predict its dependent (missing last column) using the model with the lowest MSE, RMSE, or NRMSE. Save it as a file RollNo1_RollNo2_1.csv.
8. **Disable the prediction csv file saving statement and submit this entire .ipynb file, .py file, and .csv file as a single RollNo1_RollNo2_1.zip file.**

## Exploratory Data Analysis
"""

# reading the dataset using Pandas library
train_df = pd.read_csv("https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv")

"""> read_csv is an important Pandas function to read csv files and do operations on it.

> - Generally opening a csv file through this function is easy. 

> - We need to specify the path of the csv file in our local directory. 

> - The main advantage of this function is, the csv file can be read not only locally but also using an URL.

"""

# checking the dataframe
train_df

# checking the number of rows and columns present in our dataframe
train_df.shape

"""> Clearly our dataframe contains 6082 rows and 22 columns."""

# checking a few number of rows (say top 10 rows)
train_df.head(10)

"""> The head() returns the first n rows for the object based on position.

> The default value is 5 rows 
``` python
train_df.head() # prints the top 5 rows of the dataframe
```
``` python
train_df.head(15) # prints the top 15 rows of the dataframe
```
"""

# checking whether there are any missing values or NaN values in our dataframe
train_df.isna().sum()

"""> Clearly there are no null entries and all columns are numerical."""

train_df.corr() # gives correlation matrix containing correlation with each feature

# let's visualize the correlation matrix using seaborn library  

import seaborn as sns                                     # import seaborn library
fig, ax = plt.subplots(figsize=(17,10))                   # setting the figure size 17x10
sns.heatmap(train_df.corr(), cmap='Reds', annot=True)     # plotting the correlation matrix
plt.title('Correlation Matrix',fontsize="16");            # giving title to the figure

"""> Clearly "Next_Tmax" column is strongly negative correlated with below features:
- LDAPS_RHmin
- LDAPS_CC1
- LDAPS_CC2
- LDAPS_CC3
- LDAPS_CC4
So if these features have more weights, then the "Next_Tmax"(target) has less values.<br>

> Also "Next_Tmax" column is strongly positive correlated with below features:
- LDAPS_Tmax_lapse
- Present_Tmax
- LDAPS_Tmin_lapse
- Present_Tmin
So if these features have more weights, then the "Next_Tmax"(target) has high values.
"""

# let us see the attributes of each column
train_df.describe()

"""> The describe() method is used for calculating some statistical data like percentile, mean and std of the numerical values of the DataFrame. 

> Clearly we can see that each feature has different range. So before training the model, we need to normalize the data so that all the features lie in the same range. 
"""

# Normalizing the dataset by calling the Normalize() function defined previously
train_norm_df = Normalize(train_df)
train_norm_df  # checking the normalized dataset

"""## Preparing Training and Validation Datasets"""

np.random.seed(10) # sets the train, val sets i.e don't change them by running the cell each time

#splitting 80% of the dataset(normalized dataset) into training and 20% as validation datasets
train_size = 0.8 
train, val = np.split(train_norm_df.sample(frac=1), [int(train_size * len(train_norm_df))])

# splitting the independent(X) and dependent features(t)
X_train = np.array(train[train.columns[:-1]])  # input dataset for training 
t_train = np.array(train[train.columns[-1]])   # targets of training dataset
X_val = np.array(val[val.columns[:-1]])        # input dataset for validation 
t_val = np.array(val[val.columns[-1]])         # targets of validation dataset

"""## Building L2 Model"""

def L2_model(X_train, t_train, X_val, t_val,lamda, max_iter, epsilon, lr):
 
  w = np.ones((X_train.shape[1]+1,)) # taking weights with dimensions = features of training dataset(X_train) and initialiasing with ones

  # calling the Gradient_Descent() function defined previously to optimise the loss
  w_final, train_loss_final, val_loss_final, val_NRMSE = Gradient_Descent (X_train, X_val, t_train, t_val, w, lamda, max_iter, 
                                                                           epsilon, lr,L2_Loss,L2_Gradient)

  return w_final, train_loss_final, val_loss_final, val_NRMSE

"""#### Training the L2 Model"""

lamdas = list(np.logspace(-5.0, 1, num=30))  # taking 30 values of λ between 10e-05 to 10e01 in powers of 10
lrs = 0.001                                  # choosing learning rate η
max_iter = 7000                              # choosing maximum number of iterations 
epsilon = 1e-10                              # choosing the tolerance between the previous and next loss
train_loss_l2 , val_loss_l2, val_nrmse_l2,weights_l2 = [],[],[],[] # for storing the values of losses and weights for different values of λ

for lamda in lamdas: # iterating over each λ value
    # finding the values obtained after training the model
    w_final, train_loss_final, val_loss_final, val_NRMSE = L2_model(X_train, t_train, X_val, t_val,lamda, max_iter, epsilon, lrs)
    # storing the losses and weights for plotting
    train_loss_l2.append(train_loss_final)
    val_loss_l2.append(val_loss_final)
    val_nrmse_l2.append(val_NRMSE)
    weights_l2.append(w_final)

"""#### Observations and Results of Training"""

fig = plt.figure(figsize=(12,7)); # setting the figure size
model_complexity = [1/lamda for lamda in lamdas]  # taking the list to plot model complexity

plt.subplot(121)
  # plotting the training loss over the 1/λ values 
plt.plot(model_complexity,train_loss_l2,linestyle='solid',marker = 'o',label='training_errors')
  # plotting the validation loss over the 1/λ values 
plt.plot(model_complexity,val_loss_l2,linestyle='solid',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity 1/$\lambda$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14")
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

plt.subplot(122)
  # plotting the training loss over the log10(1/λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity),train_loss_l2,linestyle='solid',marker = 'o',label='training_errors')
  # plotting the validation loss over the log10(1/λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity),val_loss_l2,linestyle='solid',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity $log_{10}(1/\lambda)$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14") 
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

  # giving a common title to the figure
plt.suptitle("Plot of training and val losses vs.model complexity",fontsize="18")
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show();

"""
> - In L2- Linear Regression, as the value of **λ increases**, the model becomes too simple and thus, it results in **Underfitting**.So as a result the training and validation losses will be very high at low values of λ's.
- And similarly, as the value of **λ decreases**, the model becomes too complex and thus, it results in **Overfitting**.
- This is clearly evident from the plots above, where there's a high loss at high values of λ (1/λ will be small) and keeps decreasing as the λ increase. 
- But there's a sweet spot where the value of λ will be optimal and thus produces good results. This sweet spot can be seen in the plot in the range around $0$ to $2$ for values of $log_{10}(\frac{1}{\lambda})$ values i.e in range of $0.01$ to $1$ for values of λ.

> So to find the best value, let's look into the zoomed version of the plot."""

fig = plt.figure(figsize=(10,7)); # setting the figure size
  # plotting the training loss over the log10(λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity)[:20],train_loss_l2[:20],linestyle='solid',marker = 'o',label='training_errors')
  # plotting the validation loss over the log10(λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity)[:20],val_loss_l2[:20],linestyle='solid',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity $log_{10}(1/\lambda)$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14")
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)
  # giving a title to the figure
plt.title("Plot of training and val losses vs.model complexity (Zoomed view)",fontsize="18")
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show();

"""> Clearly we can see that values of $log_{10}(\frac{1}{\lambda})$ between $1.5$ and $2$ are good values leading to sweet spot.

#### Finding the values for optimal λ

> We will now create a dataframe containing the values of losses and weights for different values of λ to see the values in a better way.
"""

# let's see the values of the losses at different values of λ by creating a dataframe
l2_model_df= pd.DataFrame({ 
                            "Lambdas": lamdas,
                            "Train_loss_final": train_loss_l2,
                            "Val_loss_final": val_loss_l2,
                            "Val_NRMSE": val_nrmse_l2,
                            "Weights": weights_l2
    
})
l2_model_df

"""> Since our sweet spot contains the values of $log_{10}(\frac{1}{\lambda})$ between $1.5$ and $2$ (λ is between $10^{-2}$ and $10^{-1.5}$) we will see the values of loss at these λ values."""

l2_model_df[(l2_model_df['Lambdas']<=10**(-1.5)) & (l2_model_df['Lambdas']>=10**(-2)) ] # filtering the df using the condition

"""> So the best optimal value of **λ** is **0.012690** since they are producing low values of training loss and validation loss and lead to sweet spot. 

> Clearly the optimal values are at index 15 in the dataframe. This means all the optimal values are at index 15 in the respective lists `lamdas`, `train_loss`, `val_loss`, `val_nrmse`, `weights`.
"""

# finding the weights and losses for the optimal value of λ
print("Best λ: {} ".format(lamdas[15]) )
print("Final Training loss:",train_loss_l2[15])
print("Final Validation loss:",val_loss_l2[15])
print("Final Validation NRMSE:",val_nrmse_l2[15])
print("Obtained Weights:",weights_l2[15],sep="\n")

# let's see the optimal weights obtained for each feature
weights_df = pd.DataFrame({ "Features": list(train_df.columns[:-1]), # all the features
                            "Obtained_Weights": weights_l2[15][:-1] # weights excluding bias term
    
})
weights_df

# let's see which features have more weight
weights_df.sort_values("Obtained_Weights",ascending=False)

"""> Clearly the "Next_Tmax" target column is more dependent on the "LDAPS_Tmax_lapse", "Present_Tmax", "LDAPS_RHmin"	and less dependent on "LDAPS_CC4", "LDAPS_CC1","LDAPS_WS".

## Building L1 Model
"""

def L1_model(X_train, t_train, X_val, t_val,lamda, max_iter, epsilon, lr):
 
  w = np.ones((X_train.shape[1]+1,)) # taking weights with dimensions = features of training dataset(X_train) and initialiasing with ones

  # calling the Gradient_Descent() function defined previously to optimise the loss
  w_final, train_loss_final, val_loss_final, val_NRMSE = Gradient_Descent (X_train, X_val, t_train, t_val, w, lamda, max_iter, 
                                                                           epsilon, lr,L1_Loss,L1_Gradient)

  return w_final, train_loss_final, val_loss_final, val_NRMSE

"""#### Training the L1 Model"""

lamdas = list(np.logspace(-5.0, 1, num=30))  # taking 30 values of λ between 10e-05 to 10e01 in powers of 10
lrs = 0.001                                  # choosing learning rate η
max_iter = 7000                              # choosing maximum number of iterations 
epsilon = 1e-10                              # choosing the tolerance between the previous and next loss
train_loss_l1 , val_loss_l1, val_nrmse_l1,weights_l1 = [],[],[],[] # for storing the values of losses and weights for different values of λ

for lamda in lamdas: # iterating over each λ value
    # finding the values obtained after training the model
    w_final, train_loss_final, val_loss_final, val_NRMSE = L1_model(X_train, t_train, X_val, t_val,lamda, max_iter, epsilon, lrs)
    # storing the losses and weights for plotting
    train_loss_l1.append(train_loss_final)
    val_loss_l1.append(val_loss_final)
    val_nrmse_l1.append(val_NRMSE)
    weights_l1.append(w_final)

"""#### Observations and Results of Training"""

fig = plt.figure(figsize=(12,7)); # setting the figure size
model_complexity = [1/lamda for lamda in lamdas]  # taking the list to plot model complexity

plt.subplot(121)
  # plotting the training loss over the 1/λ values
plt.plot(model_complexity,train_loss_l1,linestyle='solid',marker = 'o',label='training_errors')
  # plotting the validation loss over the 1/λ values
plt.plot(model_complexity,val_loss_l1,linestyle='solid',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity 1/$\lambda$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14")
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

plt.subplot(122)
  # plotting the training loss over the log10(1/λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity),train_loss_l1,linestyle='solid',marker = 'o',label='training_errors')
  # plotting the validation loss over the log10(1/λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity),val_loss_l1,linestyle='solid',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity $log_{10}(1/\lambda)$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14") 
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

  # giving a common title to the figure
plt.suptitle("Plot of training and val losses vs.model complexity",fontsize="18")
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show();

"""
> - In L1- Linear Regression, as the value of **λ increases**, the model becomes too simple and thus, it results in **Underfitting**.So as a result the training and validation losses will be very high at low values of λ's.
- And similarly, as the value of **λ decreases**, the model becomes too complex and thus, it results in **Overfitting**.
- This is clearly evident from the plots above, where there's a high loss at high values of λ (1/λ will be small) and keeps decresing as the λ increase. 
- But there's a sweet spot where the value of λ will be optimal and thus produces good results. This sweet spot can be seen in the plot in the range around $0$ to $2$ for values of $log_{10}(\frac{1}{\lambda})$ values i.e in range of $0.01$ to $1$ for values of λ.

> So to find the best value, let's look into the zoomed version of the plot."""

fig = plt.figure(figsize=(10,7)); # setting the figure size
  # plotting the training loss over the log10(λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity)[:20],train_loss_l1[:20],linestyle='solid',marker = 'o',label='training_errors')
  # plotting the validation loss over the log10(λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity)[:20],val_loss_l1[:20],linestyle='solid',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity $log_{10}(1/\lambda)$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14")
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

  # giving a title to the figure
plt.title("Plot of training and val losses vs.model complexity (Zoomed view)",fontsize="18")
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show();

"""> Clearly we can see that values of $log_{10}(\frac{1}{\lambda})$ between $2$ and $2.5$ are good values leading to sweet spot.

#### Finding the values for optimal λ

> We will now create a dataframe containing the values of losses and weights for different values of λ to see the values in a better way.
"""

# let's see the values of the losses at different values of λ by creating a dataframe
l1_model_df= pd.DataFrame({ 
                            "Lambdas": lamdas,
                            "Train_loss_final": train_loss_l1,
                            "Val_loss_final": val_loss_l1,
                            "Val_NRMSE": val_nrmse_l1,
                            "Weights": weights_l1
    
})
l1_model_df

"""> Since our sweet spot contains the values of $log_{10}(\frac{1}{\lambda})$ between $2$ and $2.5$ (λ is between $10^{-2.5}$ and $10^{-2}$) we will see the values of loss at these λ values."""

l1_model_df[(l1_model_df['Lambdas']<=10**(-2)) & (l1_model_df['Lambdas']>=10**(-2.5)) ] # filtering the df using the condition

"""> So the best optimal value of **λ** is **0.004894** since they are producing low values of training loss and validation loss and lead to sweet spot. 

> Clearly the optimal values are at index 13 in the dataframe. This means all the optimal values are at index 13 in the respective lists `lamdas`, `train_loss`, `val_loss`, `val_nrmse`, `weights`.
"""

# finding the weights and losses for the optimal value of λ
print("Best λ: {} ".format(lamdas[13]) )
print("Final Training loss:",train_loss_l1[13])
print("Final Validation loss:",val_loss_l1[13])
print("Final Validation NRMSE:",val_nrmse_l1[13])
print("Obtained Weights:",weights_l1[13],sep="\n")

# let's see the optimal weights obtained for each feature
weights_df = pd.DataFrame({ "Features": list(train_df.columns[:-1]), # all the features
                            "Obtained_Weights": weights_l1[13][:-1] # weights excluding bias term
    
})
weights_df

# let's see which features have more weight
weights_df.sort_values("Obtained_Weights",ascending=False)

"""> Clearly the "Next_Tmax" target column is more dependent on the "LDAPS_Tmax_lapse", "Present_Tmax", "LDAPS_RHmin"	and less dependent on "LDAPS_CC4", "LDAPS_CC1","LDAPS_WS".

## Pseudo-Inverse Method
"""

def pinv_model(X_train, t_train, X_val, t_val,lamda):
 
  w =  Pseudo_Inverse (X_train, t_train, lamda)         # finding the weights by calling the Pseudo_Inverse() function defined previously
  train_loss_final = MSE_Loss (X_train, t_train, w, lamda =0) # finding the training loss with the optimised weights 
  val_loss_final = MSE_Loss (X_val, t_val, w, lamda =0)       # finding the validation loss with the optimised weights 
  val_NRMSE = NRMSE_Metric (X_val, t_val, w, lamda=0)         # finding the validation NRMSE loss with the optimised weights

  return w, train_loss_final, val_loss_final, val_NRMSE

"""### Training the model"""

lamdas = list(np.logspace(-5.0, 1, num=30))  # taking 30 values of λ between 10e-05 to 10e01 in powers of 10
weights_pinv = []                                          # for storing the values of weights for different values of λ
train_loss_pinv , val_loss_pinv, val_nrmse_pinv = [],[],[] # for storing the values of losses for different values of λ

for lamda in lamdas: # iterating over each λ
    # finding the values obtained after training the model
  w, train_loss_final, val_loss_final, val_NRMSE = pinv_model(X_train, t_train, X_val, t_val, lamda)
  # storing the losses and weights for plotting
  weights_pinv.append(w)
  train_loss_pinv.append(train_loss_final)
  val_loss_pinv.append(val_loss_final)
  val_nrmse_pinv.append(val_NRMSE)

"""### Observations and Results of Training"""

fig = plt.figure(figsize=(12,10)); # setting the figure size
model_complexity = [1/lamda for lamda in lamdas]  # taking the list to plot model complexity

plt.subplot(221)
  # plotting the training loss over the 1/λ values 
plt.plot(model_complexity,train_loss_pinv,linestyle='solid',marker = 'o',label='training_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity 1/$\lambda$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14")
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

plt.subplot(222)
  # plotting the validation loss over the 1/λ values 
plt.plot(model_complexity,val_loss_pinv,linestyle='solid',marker = 'o',color='g',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity 1/$\lambda$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14")
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

plt.subplot(223)
  # plotting the training loss over the log10(1/λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity),train_loss_pinv,linestyle='solid',marker = 'o',label='training_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity $log_{10}(1/\lambda)$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14") 
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

plt.subplot(224)
  # plotting the validation loss over the log10(1/λ) values (since our values are in powers of 10 )
plt.plot(np.log10(model_complexity),val_loss_pinv,linestyle='solid',color='g',marker = 'o',label='val_errors')
  # labelling the x-axis
plt.xlabel("Model Complexity $log_{10}(1/\lambda)$-------->",fontsize="14")
  # labelling the y-axis
plt.ylabel("Loss------->",fontsize="14") 
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)
  # giving a common title to the figure
plt.suptitle("Plot of training and val losses vs.model complexity",fontsize="18")
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show();

"""> The  L2- Linear Regression has only one closed-form solution which is given by $w = (X^{T}X+ λI)^{-1} X^{T}t            $

- There's a sweet spot where the value of λ will be optimal and thus produces good results. This sweet spot can be seen in the plot in the range around $-1$ to $2$ for values of $log_{10}(\frac{1}{\lambda})$ values (in the training loss plot) i.e in range of $0.01$ to $10$ for values of λ.

### Finding the values for optimal λ

> We will now create a dataframe containing the values of losses and weights for different values of λ to see the values in a better way.
"""

# let's see the values of the losses at different values of λ by creating a dataframe
pinv_df= pd.DataFrame({ 
                            "Lambdas": lamdas,
                            "Train_loss_final": train_loss_pinv,
                            "Val_loss_final": val_loss_pinv,
                            "Val_NRMSE": val_nrmse_pinv,                       
                            "Weights _Pinv":weights_pinv,
                            
    
})
pinv_df

# Now let's check how the weights obtained in L2_model and the pseudo-inv weights differ.
# compute the mse between the weights_l2, weights_pinv

weights_mse=[]
for i in range(len(lamdas)): # for each lambda there's a weight
  weights_mse.append(np.mean(np.square(np.array(weights_pinv)[i] - np.array(weights_l2[i]))))

# let's add this weights_mse in the pinv_df
pinv_df.insert(loc=4, column='Weights_MSE', value=weights_mse) # insert a column of weights_mse after Val_NRMSE column

pinv_df

"""> Since our sweet spot contains the values of $log_{10}(\frac{1}{\lambda})$ between $-1$ and $2$ (λ is between $10^{-2}$ and $10^{1}$) we will see the values of loss at these λ values."""

pinv_df[(pinv_df['Lambdas']<=10**(1)) & (pinv_df['Lambdas']>=10**(-2)) ] # filtering the df using the condition

"""> So the best optimal value of **λ** is **0.012690** since they are producing low values of training loss and validation loss and lead to sweet spot. 

> Clearly the optimal values are at index 15 in the dataframe. This means all the optimal values are at index 15 in the respective lists `lamdas`, `train_loss`, `val_loss`, `val_nrmse`, `weights`,`weights_mse`.
"""

# finding the weights and losses for the optimal value of λ
print("Best λ: {} ".format(lamdas[15]) )
print("Final Training loss:",train_loss_pinv[15])
print("Final Validation loss:",val_loss_pinv[15])
print("Final Validation NRMSE:",val_nrmse_pinv[15])
print("Obtained Weights:",weights_pinv[15],sep="\n")

"""> So the best optimal value of **λ** is **0.012690**  and matches with the optimal value of λ obtained in L2 model.

## Observations and Conclusions

> Let's compare the three models and find the best model.
"""

print("------------------------L2_MODEL------------------------")
print("Best λ: {} ".format(lamdas[15]) )
print("Final Training loss:",train_loss_l2[15])
print("Final Validation loss:",val_loss_l2[15])
print("Final Validation NRMSE:",val_nrmse_l2[15])

print("========================================================")
print("------------------------L1_MODEL------------------------")
print("Best λ: {} ".format(lamdas[13]) )
print("Final Training loss:",train_loss_l1[13])
print("Final Validation loss:",val_loss_l1[13])
print("Final Validation NRMSE:",val_nrmse_l1[13])
print("========================================================")
print("------------------------PINV_MODEL------------------------")
# finding the weights for the optimal value of λ
print("Best λ: {} ".format(lamdas[15]) )
print("Final Training loss:",train_loss_pinv[15])
print("Final Validation loss:",val_loss_pinv[15])
print("Final Validation NRMSE:",val_nrmse_pinv[15])
print("========================================================")
print("Obtained Weights with L2 model:",weights_l2[15],sep="\n")
print("----------------------------------------------------------")
print("Obtained Weights using Pseudo-inv method:",weights_pinv[15],sep="\n")
print("----------------------------------------------------------")
print("Weights MSE between weights_l2 and weights_pinv:",weights_mse[15])

"""- Thus we can see that L2-Model proved to be better when compared to L1-Model. 
- The weights obtained in the L2 model are very close to the weights obtained in Pseudo-inverse method (this is the closed-form solution, so we can consider these as actual weights).
- The Mean squared error(MSE) of the error between the two weights is very low (0.0000064054) and our model turned out to be good.
- The training and validation mse loss are minimum and the model is *generalized*. So it can be used to find the predictions on new dataset. 
- Thus we find the predictions on new data using the **L2-Model** with 
  **λ: 0.01268961003167922**

## Predictions on Test data
"""

# reading the dataset using Pandas library
test_df = pd.read_csv("https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv")

# checking the dataframe
test_df

"""> Since the model is trained with normalized data and the weights obtained are also normalized, we will predict the targets by normalizing the dataset. We need to normalize the test data using the training mean and std."""

# removing the target column so that we can obtain the mean and std of the features 
df = train_df.drop('Next_Tmax',axis=1)
df

# Normalizing the dataset by the mean and std of the train dataset

    # df.mean(axis=0) computes the mean along rows i.e returns a row vector of means of each column of df
    # df.std(axis=0) computes the standard deviation along rows i.e returns a row vector of std_dev of each column of df
mean = df.mean(axis=0) 
std = df.std(axis=0)

test_norm_df = (test_df - mean)/std  # normalizing the test df
test_norm_df                         # checking the normalized dataset

""">Let's predict the target column by using the weights obtained in L2-model for best λ."""

weights = np.array(weights_l2[16]) # converting weights as numpy array
X_test = np.array(test_norm_df)    # creating testing array

X_test.shape # checking the shape of testing array

weights.shape # checking the shape of weights

y_pred = Prediction (X_test, weights)  # finding the predicted column values using Prediction() function defined previously

y_pred # checking the predicted values

y = (y_pred * train_df['Next_Tmax'].std())+ train_df['Next_Tmax'].mean() # denormalizing the predicted values using the train data mean, std

y # check predicted values(denormalized)

# adding the predicted target column to the normalized dataset 
test_df['Next_Tmax'] = y
test_df

# saving this dataset as a csv file
#test_norm_df.to_csv('213070015_213079003_1.csv', index=False)

"""#**... Part 2 ends.**

1. Write the name or roll no.s of friends from outside your group with whom you discussed the assignment here (no penalty for mere discussion without copying code): None
2. Write the links of sources on the internet referred here (no penalty for mere consultation without copying code): 

https://www.coursera.org/lecture/predictive-modeling-model-fitting-regression-analysis/model-generalization-ZBgh2

https://xavierbourretsicotte.github.io/intro_ridge.html

https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning

https://www.youtube.com/channel/UCbdAZq799T9mxzRyGz3XYkA/playlists
"""